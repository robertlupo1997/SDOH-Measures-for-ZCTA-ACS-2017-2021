getwd()
library(datasets)
library(iris)
head(iris)
?drop_na
library(tidyr)
install.packages("tidyr")
# Data Manipulation and Analysis
library(dplyr)       # For data manipulation
# Create a vector of package names
packages <- c("dplyr", "tidyr", "readr", "purrr", "stringr", "forcats",
"ggplot2", "ggthemes", "ggpubr", "plotly", "ggmap", "leaflet",
"stats", "lmtest", "survival", "nlme", "MASS", "caret",
"randomForest", "e1071", "xgboost", "nnet", "glmnet", "forecast",
"tseries", "zoo", "rmarkdown", "shiny", "pacman", "data.table",
"lubridate", "knitr")
# Install packages if not already installed
install.packages(setdiff(packages, rownames(installed.packages())))
# Load all the installed packages
pacman::p_load_all(packages)
# Load all the installed packages
invisible(sapply(packages, library, character.only = TRUE))
install.packages("usethis")
install.packages("gh")
library(usethis)
library(gh)
usethis::use_git_config(user.name = "robertlupo1997", user.email = "treylupo1197@gmail.com")
setwd("C:/Users/treyl/Downloads/Data Analysis/GitHub/SDOH-Measures-for-ZCTA-ACS-2017-2021")
usethis::use_git()
usethis::use_github()
install.packages("gert")
library(gert)
clear
data <- read.csv("bumh-rgsq_version_37.csv")
#check missing data
# Assuming 'data' is your dataset
library(naniar)
install.packages(naniar)
# Install the naniar package
install.packages("naniar")
# Load the naniar package
library(naniar)
# Assuming 'data' is your dataset, check for missing data
# Visualize the missing data
vis_miss(data)
setwd("C:/Users/Trey/Documents/GitHub/SDOH-Measures-for-ZCTA-ACS-2017-2021")
data <- read.csv("bumh-rgsq_version_37.csv")
#check missing data
# Assuming 'data' is your dataset
library(naniar)
# Visualize missing values with downsampling
vis_miss(dplyr::slice_sample(data, n = 1000))
# Or suppress the warning for large data
vis_miss(data, warn_large_data = FALSE)
library(naniar)
# Assuming 'data' is your dataset
missing_rows <- data[!complete.cases(data), ]
# View the first few rows of the new data frame
head(missing_rows)
# Since the dataset is large enough, we can remove the missing data entries
# Remove rows with missing values
clean_data <- na.omit(data)
# Check the structure of the cleaned data
str(clean_data)
gc()
# Abstract: A brief summary of the project.
abstract <- "This project aims to analyze Social Determinants of Health (SDOH) measures for ZIP Code Tabulation Areas (ZCTA) using the American Community Survey (ACS) data from 2017 to 2021. By cleaning and exploring the dataset, we seek to uncover trends and insights that can inform data-driven decision-making. The analysis employs various statistical techniques and predictive modeling to evaluate the impact of different measures on health outcomes."
cat(abstract)
# Introduction: Provide context and state the research question or hypothesis.
introduction <- "Social Determinants of Health (SDOH) are non-medical factors that influence health outcomes. These include socioeconomic status, education, neighborhood and physical environment, employment, social support networks, and access to healthcare. This project focuses on analyzing SDOH measures across various ZIP Code Tabulation Areas (ZCTAs) using data from the American Community Survey (ACS) between 2017 and 2021. The goal is to identify key factors that impact health outcomes and understand the geographical distribution of these determinants."
cat(introduction)
# Data Description: Describe the dataset and its source.
data_description <- "The dataset used in this project is sourced from the American Community Survey (ACS) and provides SDOH measures for ZIP Code Tabulation Areas (ZCTAs) from 2017 to 2021. The dataset includes variables such as data_value, margin of error (moe), total population, latitude, longitude, and various categorical indicators. The data was cleaned to handle missing values and ensure consistency across measures."
cat(data_description)
# Methodology: Explain the statistical techniques and models applied.
methodology <- "The methodology for this project includes several steps:
1. Data Cleaning: Handling missing values and ensuring data consistency.
2. Exploratory Data Analysis (EDA): Generating summary statistics and visualizations to understand data distributions and relationships.
3. Correlation Analysis: Identifying relationships between numerical variables.
4. Predictive Modeling: Developing models to predict health outcomes based on SDOH measures. Models considered include linear regression, logistic regression, and support vector machines.
5. Model Evaluation: Assessing model performance using metrics such as R-squared, mean squared error, and accuracy, and employing cross-validation techniques."
cat(methodology)
# Results: Present findings from the analysis.
results <- "The analysis revealed several key insights:
- The distribution of key numerical variables such as data_value, moe, and total population.
- Correlation analysis showed significant relationships between data_value, latitude, longitude, and total population.
- Predictive modeling identified important SDOH measures that influence health outcomes. The models were evaluated and validated to ensure reliability."
cat(results)
# Discussion: Interpret the results and their implications.
discussion <- "The findings suggest that certain SDOH measures have a significant impact on health outcomes. For instance, areas with higher total populations and specific geographical locations tend to have different health outcomes. These insights can inform public health strategies and resource allocation to address disparities. However, the analysis is limited by the quality and granularity of the data, and further research could expand on these findings by incorporating additional variables and more granular data."
cat(discussion)
# Conclusion: Summarize key findings and suggest future research.
conclusion <- "In conclusion, this project highlights the importance of SDOH measures in understanding health outcomes. By analyzing ACS data from 2017 to 2021, we identified key factors that influence health and provided insights that can guide public health decision-making. Future research could explore more detailed datasets and consider additional variables to further refine these findings."
cat(conclusion)
# References: Cite sources and data used in the project.
references <- "American Community Survey (ACS) data, 2017-2021."
cat(references)
# Load necessary libraries
library(tidyverse)
library(data.table)
library(naniar)
library(ggcorrplot)
library(maps)
# Set working directory
setwd("C:/Users/Trey/Documents/GitHub/SDOH-Measures-for-ZCTA-ACS-2017-2021")
# Load the dataset
data <- fread("bumh-rgsq_version_37.csv")
# Convert to data.table
setDT(data)
# Check the structure of the dataset
str(data)
# Check for missing values
vis_miss(data, warn_large_data = FALSE)
# Fill missing values with the mean of their respective 'measure' groups
data[, c("data_value", "moe") := lapply(.SD, function(x) {
ifelse(is.na(x), ave(x, data$measure, FUN = function(y) mean(y, na.rm = TRUE)), x)
}), .SDcols = c("data_value", "moe")]
# Verify that there are no missing values left
missing_counts <- data[, lapply(.SD, function(x) sum(is.na(x)))]
print(missing_counts)
# Convert data types if necessary
data[, year := as.character(year)]
data[, locationname := as.character(locationname)]
data[, locationid := as.character(locationid)]
View(missing_counts)
#EDA
# Load necessary libraries
required_packages <- c("data.table", "naniar", "ggcorrplot", "maps", "ggplot2", "dplyr")
# Install missing packages
installed_packages <- rownames(installed.packages())
for (pkg in required_packages) {
if (!(pkg %in% installed_packages)) {
install.packages(pkg)
}
}
# Load libraries
lapply(required_packages, library, character.only = TRUE)
# Increase memory limit (Windows only)
if (.Platform$OS.type == "windows") {
memory.limit(size = 16000)
}
# Load the dataset
data <- fread("bumh-rgsq_version_37.csv")
# Check for missing data in a sample of the dataset
vis_miss(dplyr::slice_sample(data, n = 1000))
# Fill missing values with the mean of their respective 'measure' groups
data[, c("data_value", "moe") := lapply(.SD, function(x) {
mean_val <- mean(x, na.rm = TRUE)
ifelse(is.na(x), mean_val, x)
}), .SDcols = c("data_value", "moe")]
# Verify that there are no missing values left
missing_counts <- data[, lapply(.SD, function(x) sum(is.na(x)))]
# Print missing_counts
print(missing_counts)
# Exploratory Data Analysis (EDA)
# Display basic information about the dataset
glimpse(data)
# Summary statistics for numerical columns
summary(data[, .SD, .SDcols = sapply(data, is.numeric)])
# Visualize the distribution of key numerical variables
data[, .(data_value, moe, totalpopulation)] %>%
melt(measure.vars = c("data_value", "moe", "totalpopulation"), variable.name = "variable", value.name = "value") %>%
ggplot(aes(x = value)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_wrap(~variable, scales = "free_x") +
theme_minimal() +
labs(title = "Distribution of Key Numerical Variables")
# Correlation matrix
correlation_matrix <- data[, .(data_value, moe, latitude, longitude, totalpopulation)] %>%
cor(use = "complete.obs")
# Visualize the correlation matrix
ggcorrplot(correlation_matrix, method = "circle", lab = TRUE) +
labs(title = "Correlation Matrix")
# Visualize geographical distribution
world_map <- map_data("world")
ggplot() +
geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = "lightgray") +
geom_point(data = data, aes(x = longitude, y = latitude, color = totalpopulation), alpha = 0.7) +
theme_minimal() +
labs(title = "Geographical Distribution of Data Points",
x = "Longitude", y = "Latitude", color = "Total Population")
# Violin plot for 'data_value' across different 'measure'
data[, .(measure, data_value)] %>%
ggplot(aes(x = measure, y = data_value, fill = measure)) +
geom_violin() +
theme_minimal() +
labs(title = "Data Value Across Different Measures",
x = "Measure", y = "Data Value") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
View(data)
```{r violin_plot, fig.width=12, fig.height=8}
# Violin plot for 'data_value' across different 'measure'
data[, .(measure, data_value)] %>%
ggplot(aes(x = measure, y = data_value, fill = measure)) +
geom_violin() +
theme_minimal() +
labs(title = "Data Value Across Different Measures",
x = "Measure", y = "Data Value") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#EDA
# Load necessary libraries
required_packages <- c("data.table", "naniar", "ggcorrplot", "maps", "ggplot2", "dplyr")
# Install missing packages
installed_packages <- rownames(installed.packages())
for (pkg in required_packages) {
if (!(pkg %in% installed_packages)) {
install.packages(pkg)
}
}
# Load libraries
lapply(required_packages, library, character.only = TRUE)
# Increase memory limit (Windows only)
if (.Platform$OS.type == "windows") {
memory.limit(size = 16000)
}
# Load the dataset
data <- fread("bumh-rgsq_version_37.csv")
# Check for missing data in a sample of the dataset
vis_miss(dplyr::slice_sample(data, n = 1000))
# Fill missing values with the mean of their respective 'measure' groups
data[, c("data_value", "moe") := lapply(.SD, function(x) {
mean_val <- mean(x, na.rm = TRUE)
ifelse(is.na(x), mean_val, x)
}), .SDcols = c("data_value", "moe")]
# Verify that there are no missing values left
missing_counts <- data[, lapply(.SD, function(x) sum(is.na(x)))]
# Print missing_counts
print(missing_counts)
# Exploratory Data Analysis (EDA)
# Display basic information about the dataset
glimpse(data)
# Summary statistics for numerical columns
summary(data[, .SD, .SDcols = sapply(data, is.numeric)])
# Visualize the distribution of key numerical variables
data[, .(data_value, moe, totalpopulation)] %>%
melt(measure.vars = c("data_value", "moe", "totalpopulation"), variable.name = "variable", value.name = "value") %>%
ggplot(aes(x = value)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_wrap(~variable, scales = "free_x") +
theme_minimal() +
labs(title = "Distribution of Key Numerical Variables")
# Correlation matrix
correlation_matrix <- data[, .(data_value, moe, latitude, longitude, totalpopulation)] %>%
cor(use = "complete.obs")
# Visualize the correlation matrix
ggcorrplot(correlation_matrix, method = "circle", lab = TRUE) +
labs(title = "Correlation Matrix")
# Visualize geographical distribution
world_map <- map_data("world")
ggplot() +
geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = "lightgray") +
geom_point(data = data, aes(x = longitude, y = latitude, color = totalpopulation), alpha = 0.7) +
theme_minimal() +
labs(title = "Geographical Distribution of Data Points",
x = "Longitude", y = "Latitude", color = "Total Population")
# Violin plot for 'data_value' across different 'measure'
data[, .(measure, data_value)] %>%
ggplot(aes(x = measure, y = data_value, fill = measure)) +
geom_violin() +
theme_minimal() +
labs(title = "Data Value Across Different Measures",
x = "Measure", y = "Data Value") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Violin plot for 'data_value' across different 'measure'
data[, .(measure, data_value)] %>%
ggplot(aes(x = measure, y = data_value, fill = measure)) +
geom_violin() +
theme_minimal() +
labs(title = "Data Value Across Different Measures",
x = "Measure", y = "Data Value") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#EDA
# Load necessary libraries
required_packages <- c("data.table", "naniar", "ggcorrplot", "maps", "ggplot2", "dplyr")
# Install missing packages
installed_packages <- rownames(installed.packages())
for (pkg in required_packages) {
if (!(pkg %in% installed_packages)) {
install.packages(pkg)
}
}
# Load libraries
lapply(required_packages, library, character.only = TRUE)
# Increase memory limit (Windows only)
if (.Platform$OS.type == "windows") {
memory.limit(size = 16000)
}
# Load the dataset
data <- fread("bumh-rgsq_version_37.csv")
# Check for missing data in a sample of the dataset
vis_miss(dplyr::slice_sample(data, n = 1000))
# Fill missing values with the mean of their respective 'measure' groups
data[, c("data_value", "moe") := lapply(.SD, function(x) {
mean_val <- mean(x, na.rm = TRUE)
ifelse(is.na(x), mean_val, x)
}), .SDcols = c("data_value", "moe")]
# Verify that there are no missing values left
missing_counts <- data[, lapply(.SD, function(x) sum(is.na(x)))]
# Print missing_counts
print(missing_counts)
# Exploratory Data Analysis (EDA)
# Display basic information about the dataset
glimpse(data)
# Summary statistics for numerical columns
summary(data[, .SD, .SDcols = sapply(data, is.numeric)])
# Visualize the distribution of key numerical variables
data[, .(data_value, moe, totalpopulation)] %>%
melt(measure.vars = c("data_value", "moe", "totalpopulation"), variable.name = "variable", value.name = "value") %>%
ggplot(aes(x = value)) +
geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
facet_wrap(~variable, scales = "free_x") +
theme_minimal() +
labs(title = "Distribution of Key Numerical Variables")
# Correlation matrix
correlation_matrix <- data[, .(data_value, moe, latitude, longitude, totalpopulation)] %>%
cor(use = "complete.obs")
# Visualize the correlation matrix
ggcorrplot(correlation_matrix, method = "circle", lab = TRUE) +
labs(title = "Correlation Matrix")
# Visualize geographical distribution
world_map <- map_data("world")
ggplot() +
geom_polygon(data = world_map, aes(x = long, y = lat, group = group), fill = "lightgray") +
geom_point(data = data, aes(x = longitude, y = latitude, color = totalpopulation), alpha = 0.7) +
theme_minimal() +
labs(title = "Geographical Distribution of Data Points",
x = "Longitude", y = "Latitude", color = "Total Population")
# Visualize geographical distribution focused on the USA
usa_map <- map_data("state")
ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group), fill = "lightgray") +
geom_point(data = data, aes(x = longitude, y = latitude, color = totalpopulation), alpha = 0.7) +
coord_fixed(1.3, xlim = c(-125, -66.5), ylim = c(24.5, 49.5)) +
theme_minimal() +
labs(title = "Geographical Distribution of Data Points in the USA",
x = "Longitude", y = "Latitude", color = "Total Population")
